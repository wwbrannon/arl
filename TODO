performance issues:
-------------------------------------------------
o) check that the benchmarks run correctly and run/commit bench/profile

o) should we inject helpers into the environment rather than the generated
   scripts themselves? pros of environment: slightly faster. maybe more robust.
   cons: compiled programs aren't self-contained. (but would they be anyway?)

o) compiler vignette

o) where in here do we expose things user code can modify and break compilation?

o) should we write any of these compiled versions to disk a la __pycache__?
o) probably room to further optimize the generated expressions

compiler opt ideas
  1. Macro expansion cache

  - Key: (expr, env macro registry version)
  - Value: expanded AST + src metadata
  - Big wins for repeated macro-heavy code and module loads.

  2. Compile cache

  - Key: (expanded AST, env/compiler options)
  - Value: compiled R expression
  - Useful for REPL, repeated evaluations, and stdlib module loads.

  3. Module load cache

  - Already partly present, but could cache compiled bodies per module file hash to skip parse/expand/compile on repeated loads.

  4. Constant folding & literal hoisting

  - Precompute pure literal calls (e.g., (+ 1 2)) and hoist immutable literals into temps.

  5. Function body compilation memoization

  - Cache compiled bodies for lambdas created from identical source to reduce closure compilation overhead in macro expansions.

  11. Inline simple runtime helpers

  - For tiny helpers (.rye_true_p, etc.) to reduce call overhead.

  12. Specialized fast paths for hot forms

  - if, begin, and/or, lambda bodies compiled into more direct R idioms.

o) have opus or some such critique the architecture and make suggestions
o) other performance optimizations: use profiling and benchmarking

test audits:
-------------------------------------------------
o) fix the minor issues found last time from opus auditing the tests

o) have opus audit the tests again:
```
Take a look at the tests in tests/, maybe especially the native ones in
tests/native/. Keep in mind that the language being tested is supposed to be a
lisp, generally scheme-like, but implemented on top of R. So that makes for a
few peculiarities -- R's type system, no tail call optimization, the whole
"r/call" apparatus, etc.

Given that, let's critique these tests. Is there any behavior they implement
that they clearly shouldn't? Mistaken behavior of the interpreter we have now
that's been hardcoded into the tests? You're looking for things that clearly
don't make sense and aren't how an R-based lisp should behave. Use your
judgment!
```
or similar

documentation updates:
-------------------------------------------------
o) update / systematics docs: make a plan

small stuff:
--------------------------------------
o) minor pkgdown warning about site url
o) run lints and coverage, see what to do

