performance issues:
-------------------------------------------------
o) main perf bottleneck: evaluator is a dynamic, recursive AST walker. per rye
   call, it creates way too many stack frames and does way too much
   evaluation-time work. to fix this, we want to add an AST-to-R compiler and
   replace the evaluator with just a call to base::eval(). this means our
   overhead is all at macroexpansion + transpilation time and stacks are
   shorter / recursion can safely be deeper (depth is determined by the depth
   of the AST, not the runtime call graph). there's of course still evaluation
   but R's eval() is in C and is highly optimized, much more efficient than
   anything we could possibly do

o) still some missing compiler features and failing tests
o) eliminate the current evaluator entirely

o) recursion-related FIXMEs (reenable some code) in benchmarks after evaluator
   improvements

o) should we inject helpers into the environment rather than the generated
   scripts themselves? pros of environment: slightly faster. maybe more robust.
   cons: compiled programs aren't self-contained. (but would they be anyway?)

o) further speedups:
    o) caching in the compiler? or write compiled files a la __pycache__?
    o) probably room to further optimize the generated expressions

o) compiler vignette
o) where in here do we expose things user code can modify and break compilation?
o) have opus or some such critique the architecture and make suggestions

o) other performance optimizations: use profiling and benchmarking

test audits:
-------------------------------------------------
o) fix the minor issues found last time from opus auditing the tests

o) have opus audit the tests again:
```
Take a look at the tests in tests/, maybe especially the native ones in
tests/native/. Keep in mind that the language being tested is supposed to be a
lisp, generally scheme-like, but implemented on top of R. So that makes for a
few peculiarities -- R's type system, no tail call optimization, the whole
"r/call" apparatus, etc.

Given that, let's critique these tests. Is there any behavior they implement
that they clearly shouldn't? Mistaken behavior of the interpreter we have now
that's been hardcoded into the tests? You're looking for things that clearly
don't make sense and aren't how an R-based lisp should behave. Use your
judgment!
```
or similar

documentation updates:
-------------------------------------------------
o) update / systematics docs: make a plan

small stuff:
--------------------------------------
o) minor pkgdown warning about site url
