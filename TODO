test audits:
-------------------------------------------------
o) fix the minor issues found last time from opus auditing the tests

o) have opus audit the tests again:
```
Take a look at the tests in tests/, maybe especially the native ones in tests/native/. Keep in mind that the language being tested is supposed to be a lisp, generally scheme-like, but implemented on top of R. So that makes for a few peculiarities -- R's type system, no tail call optimization, the whole "r/call" apparatus, etc.

Given that, let's critique these tests. Is there any behavior they implement that they clearly shouldn't? Mistaken behavior of the interpreter we have now that's been hardcoded into the tests? You're looking for things that clearly don't make sense and aren't how an R-based lisp should behave. Use your judgment!
```
or similar

performance issues:
-------------------------------------------------
o) the evaluator is a dynamic, recursive AST walker and creates way too many stack frames per rye call.
  o) this is the main performance bottleneck.
  o) option 1: replace it with an AST-to-R compiler which will also kill off the translator program we have now.
  o) option 2: we can walk the AST more carefully, nonrecursively, maybe by explicitly treating the R language object we get as a graph. (evaluate the AST nodes in a postorder traversal with an explicit stack and caching, right? any scoping subtleties this misses?) this is a much smaller change.
  o) option 3: can we build the input AST a bit differently or provide helpers so that the entire evaluator is just a call to R's eval()?
  o) recursion-related FIXMEs (reenable some code) in benchmarks after evaluator improvements

o) other performance optimizations: use profiling and benchmarking

documentation updates:
-------------------------------------------------
o) update / systematics docs: make a plan

small stuff:
--------------------------------------
o) minor pkgdown warning about site url
