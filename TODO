performance issues:
-------------------------------------------------
o) let's also save the generated R code as a .R script for debugging / inspectability

o) probably room to further optimize the generated expressions

o) compiler vignette

o) where in here do we expose things user code can modify and break compilation?

o) have opus or some such critique the architecture and make suggestions
o) other performance optimizations: use profiling and benchmarking

test audits:
-------------------------------------------------
o) fix the minor issues found last time from opus auditing the tests

o) have opus audit the tests again:
```
Take a look at the tests in tests/, maybe especially the native ones in
tests/native/. Keep in mind that the language being tested is supposed to be a
lisp, generally scheme-like, but implemented on top of R. So that makes for a
few peculiarities -- R's type system, no tail call optimization, the whole
"r/call" apparatus, etc.

Given that, let's critique these tests. Is there any behavior they implement
that they clearly shouldn't? Mistaken behavior of the interpreter we have now
that's been hardcoded into the tests? You're looking for things that clearly
don't make sense and aren't how an R-based lisp should behave. Use your
judgment!
```
or similar

documentation updates:
-------------------------------------------------
o) update / systematize docs: make a plan

small stuff:
--------------------------------------
o) general code quality / duplication pass

o) minor pkgdown warning about site url
o) run lints and coverage, see what to do
