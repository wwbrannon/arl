performance issues:
-------------------------------------------------
o) a make clean addition to remove the caches
o) shrink profvis output format
o) remove huge html from existing unpushed commits
o) let's generate the html though
o) is digest one of the R recommended packages? does it ship with the interpreter? we need to either remove use of it or update DESCRIPTION
o) what about version checks for compiled language rather than env? do we do this?
o) do we strip the version out of the env passed back to the user?

o) where in here do we expose things user code can modify and break compilation? for example: all the .rye_ variables, the pacakge version stored into a cached env

o) probably room to further optimize the generated expressions

o) compiler vignette

o) have opus or some such critique the architecture and make suggestions
o) other performance optimizations: use profiling and benchmarking

test audits:
-------------------------------------------------
o) fix the minor issues found last time from opus auditing the tests

o) have opus audit the tests again:
```
Take a look at the tests in tests/, maybe especially the native ones in
tests/native/. Keep in mind that the language being tested is supposed to be a
lisp, generally scheme-like, but implemented on top of R. So that makes for a
few peculiarities -- R's type system, no tail call optimization, the whole
"r/call" apparatus, etc.

Given that, let's critique these tests. Is there any behavior they implement
that they clearly shouldn't? Mistaken behavior of the interpreter we have now
that's been hardcoded into the tests? You're looking for things that clearly
don't make sense and aren't how an R-based lisp should behave. Use your
judgment!
```
or similar

documentation updates:
-------------------------------------------------
o) update / systematize docs: make a plan

small stuff:
--------------------------------------
o) general code quality / duplication pass

o) minor pkgdown warning about site url
o) run lints and coverage, see what to do
